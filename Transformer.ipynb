{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12422548,"sourceType":"datasetVersion","datasetId":7835271},{"sourceId":12422552,"sourceType":"datasetVersion","datasetId":7835274},{"sourceId":12422555,"sourceType":"datasetVersion","datasetId":7835277},{"sourceId":12425155,"sourceType":"datasetVersion","datasetId":7837042},{"sourceId":465868,"sourceType":"modelInstanceVersion","modelInstanceId":375864,"modelId":396634}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SET-UP","metadata":{}},{"cell_type":"code","source":"import os\n\n# IF THE DIRECTORY ALREADY EXISTS (ALREADY IMPORTED)\nif not os.path.exists('/kaggle/working/taming-transformers'):\n    print(\"Cloning taming-transformers repository...\")\n    !git clone https://github.com/CompVis/taming-transformers.git\n    print(\"Repository cloned successfully!\")\nelse:\n    print(\"Repository already exists, skipping clone.\")\n\n# MOVE TO THE DIRECTORY (KAGGLE STARTS FROM THE ROOT DIRECTORY OF WORKING)\n%cd taming-transformers\n\n\n\n# 1. Downgrade PyTorch alle versioni compatibili\n!pip install torch # ==1.7.0 \n!pip install torchvision # ==0.8.1\n\n# 2. Install PyTorch Lightning versione specifica\n!pip install pytorch-lightning # ==1.0.8\n\n# 3. Install altre dependencies con versioni esatte\n!pip install albumentations # ==0.4.3\n!pip install opencv-python #==4.1.2.30\n!pip install omegaconf # ==2.0.0\n!pip install einops # ==0.3.0\n!pip install transformers==4.3.1\n\n!pip install imageio # ==2.9.0\n!pip install streamlit # >=0.73.1\n\n#INFERED\n!pip install tensorboard # ==2.2.0\n\n# 4. Install il package\n!pip install -e .\n\nprint (\"ready final\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:36:44.484406Z","iopub.execute_input":"2025-07-20T07:36:44.485129Z","iopub.status.idle":"2025-07-20T07:39:04.085762Z","shell.execute_reply.started":"2025-07-20T07:36:44.485104Z","shell.execute_reply":"2025-07-20T07:39:04.084995Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VQGAN CREATION","metadata":{}},{"cell_type":"code","source":"# ------------------------------------------------------ \n#                   IMPORTS & SET-UP\n# ------------------------------------------------------ \n\nimport taming                     # Import Taming Transformers (for Image Generation)\nimport torch                      # PyTorch\nimport torchvision                # Pytorch for CV\nimport os\n\n#FIX ISSUES -------------------------------------------------------\nutils_file = '/kaggle/working/taming-transformers/taming/data/utils.py'\n\n# Read the file\nwith open(utils_file, 'r') as f:\n    content = f.read()\n\n# Replace the problematic import\ncontent = content.replace(\n    'from torch._six import string_classes',\n    'string_classes = str'\n)\n\n# Write back the fixed file\nwith open(utils_file, 'w') as f:\n    f.write(content)\n\nprint(\"Fixed torch._six import issue!\")\n\n# ----------------------------------------------------------------------\n\nimport urllib.request\n\n# Create models directory\nos.makedirs('/kaggle/working/models', exist_ok=True)\n\n# Download VQGAN ImageNet f16 model\nmodel_urls = {\n    'vqgan_imagenet_f16_16384.yaml': 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1',\n    'vqgan_imagenet_f16_16384.ckpt': 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1'\n}\n\nfor filename, url in model_urls.items():\n    filepath = f'/kaggle/working/models/{filename}'\n    if not os.path.exists(filepath):\n        print(f\"Downloading {filename}...\")\n        urllib.request.urlretrieve(url, filepath)\n        print(f\"Downloaded {filename}\")\n    else:\n        print(f\"{filename} already exists\")\n\n# ----------------------------------------------------------------------\n\nimport yaml\nfrom taming.models.vqgan import VQModel\n\ndef load_vqgan_model(config_path, checkpoint_path):\n    # Load config with regular yaml\n    with open(config_path, 'r') as f:\n        config = yaml.safe_load(f)\n    \n    # Initialize model\n    model = VQModel(**config['model']['params'])\n    \n    # Load checkpoint\n    checkpoint = torch.load(checkpoint_path, map_location='cpu',weights_only=False)\n    model.load_state_dict(checkpoint['state_dict'], strict=False)\n    \n    model.eval()\n    return model\n\n# Load the model\nconfig_path = '/kaggle/working/models/vqgan_imagenet_f16_16384.yaml'\ncheckpoint_path = '/kaggle/working/models/vqgan_imagenet_f16_16384.ckpt'\n\nvqgan_model = load_vqgan_model(config_path, checkpoint_path)\nprint(\"Model loaded!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:39:04.087161Z","iopub.execute_input":"2025-07-20T07:39:04.087370Z","iopub.status.idle":"2025-07-20T07:39:20.260640Z","shell.execute_reply.started":"2025-07-20T07:39:04.087350Z","shell.execute_reply":"2025-07-20T07:39:20.259982Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CONFIGURATION","metadata":{}},{"cell_type":"code","source":"from omegaconf import OmegaConf\n\n# Complete configuration\nconfig = OmegaConf.create({\n    \"first_stage_config\": {\n        \"target\": \"taming.models.vqgan.VQModel\",\n        \"params\": {\n            \"ckpt_path\": \"/kaggle/working/models/vqgan_imagenet_f16_16384.ckpt\",\n            \"embed_dim\": 256,\n            \"n_embed\": 16384,\n            \"ddconfig\": {\n                \"double_z\": False,\n                \"z_channels\": 256,\n                \"resolution\": 256,\n                \"in_channels\": 3,\n                \"out_ch\": 3,\n                \"ch\": 128,\n                \"ch_mult\": [1, 1, 2, 2, 4],\n                \"num_res_blocks\": 2,\n                \"attn_resolutions\": [16],\n                \"dropout\": 0.0\n            },\n            \"lossconfig\": {\n                \"target\": \"taming.modules.losses.DummyLoss\"\n            }\n        }\n    },\n    \"cond_stage_config\": \"__is_first_stage__\",  # Use same VQ-GAN for both images\n    \"transformer_config\": {\n        \"target\": \"taming.modules.transformer.mingpt.GPT\",\n        \"params\": {\n            \"vocab_size\": 16384,  # This matches your codebook size\n            \"block_size\": 512,    # Sequence length (for 16x16 = 256 tokens x2)\n            \"n_layer\": 12,\n            \"n_head\": 8,\n            \"n_embd\": 512,\n            # --- ADDED DROPOUT FOR REGULARIZATION ---\n            \"embd_pdrop\": 0.2,    # Dropout on embeddings\n            \"resid_pdrop\": 0.2,   # Dropout on residual connections\n            \"attn_pdrop\": 0.2     # Dropout on attention weights\n        }\n    }\n})\n\nprint(\"Configuration created!\")\nprint(f\"VQ-GAN codebook size: {config.first_stage_config.params.n_embed}\")\nprint(f\"Transformer vocab size: {config.transformer_config.params.vocab_size}\")\n\n# Define device first\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:39:20.261330Z","iopub.execute_input":"2025-07-20T07:39:20.261758Z","iopub.status.idle":"2025-07-20T07:39:20.312127Z","shell.execute_reply.started":"2025-07-20T07:39:20.261738Z","shell.execute_reply":"2025-07-20T07:39:20.311404Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TOP K","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch import Tensor\n\ndef top_k_top_p_filtering(\n    logits: Tensor,\n    top_k: int = 0,\n    top_p: float = 1.0,\n    filter_value: float = -float(\"Inf\"),\n    min_tokens_to_keep: int = 1,\n) -> Tensor:\n    \"\"\"Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n    Args:\n        logits: logits distribution shape (batch size, vocabulary size)\n        if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n        if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n            Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n        Make sure we keep at least min_tokens_to_keep per batch example in the output\n    From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n    \"\"\"\n    if top_k > 0:\n        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check\n        # Remove all tokens with a probability less than the last token of the top-k\n        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n        logits[indices_to_remove] = filter_value\n\n    if top_p < 1.0:\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n        sorted_indices_to_remove = cumulative_probs > top_p\n        if min_tokens_to_keep > 1:\n            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n        # Shift the indices to the right to keep also the first token above the threshold\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n        sorted_indices_to_remove[..., 0] = 0\n\n        # scatter sorted tensors to original indexing\n        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n        logits[indices_to_remove] = filter_value\n    return logits\n\n# Inject the real function into transformers module\nimport transformers\ntransformers.top_k_top_p_filtering = top_k_top_p_filtering\n\nprint(\"‚úÖ Real top_k_top_p_filtering function added to transformers!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:39:20.313879Z","iopub.execute_input":"2025-07-20T07:39:20.314080Z","iopub.status.idle":"2025-07-20T07:39:20.321031Z","shell.execute_reply.started":"2025-07-20T07:39:20.314064Z","shell.execute_reply":"2025-07-20T07:39:20.320256Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VQGAN - LOAD","metadata":{}},{"cell_type":"code","source":"from taming.models.cond_transformer import Net2NetTransformer\n\n\n# Load checkpoint with explicit weights_only=False\nvqgan_state = torch.load(\"/kaggle/working/models/vqgan_imagenet_f16_16384.ckpt\", \n                        map_location=device, \n                        weights_only=False)\n\n# Modify config to not load checkpoint automatically\nconfig_no_ckpt = config.copy()\nconfig_no_ckpt.first_stage_config.params.ckpt_path = None\n\n\n# Create model without loading checkpoint\nmodel = Net2NetTransformer(\n    transformer_config=config.transformer_config,\n    first_stage_config=config_no_ckpt.first_stage_config,\n    cond_stage_config=config.cond_stage_config,\n    first_stage_key=\"satellite\",\n    cond_stage_key=\"ground\",\n    unconditional=False\n)\n\n# Manually load the VQ-GAN weights\nmodel.first_stage_model.load_state_dict(vqgan_state[\"state_dict\"], strict=False)\n\nprint(\"Model created and VQ-GAN weights loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:39:20.321754Z","iopub.execute_input":"2025-07-20T07:39:20.321921Z","iopub.status.idle":"2025-07-20T07:39:22.878278Z","shell.execute_reply.started":"2025-07-20T07:39:20.321907Z","shell.execute_reply":"2025-07-20T07:39:22.877698Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FREEZE","metadata":{}},{"cell_type":"code","source":"#Freeze the VQ-GAN parameters\nfor param in model.first_stage_model.parameters():\n    param.requires_grad = False\n\n# Since cond_stage_model is the same as first_stage_model, it's already frozen\n\n# Verify the freeze worked\nvqgan_params = sum(p.numel() for p in model.first_stage_model.parameters() if p.requires_grad)\ntransformer_params = sum(p.numel() for p in model.transformer.parameters() if p.requires_grad)\n\nprint(\"After freezing VQ-GAN:\")\nprint(f\"Trainable VQ-GAN parameters: {vqgan_params}\")\nprint(f\"Trainable Transformer parameters: {transformer_params}\")\nprint(f\"Total trainable parameters: {vqgan_params + transformer_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:39:22.879076Z","iopub.execute_input":"2025-07-20T07:39:22.879745Z","iopub.status.idle":"2025-07-20T07:39:22.886659Z","shell.execute_reply.started":"2025-07-20T07:39:22.879717Z","shell.execute_reply":"2025-07-20T07:39:22.885876Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CSV Train-Test Split Fixing","metadata":{}},{"cell_type":"code","source":"import csv\nimport os\nfrom pathlib import Path\n\ndef check_csv_structure(csv_path):\n    \"\"\"\n    Check the structure of the CSV file and print sample rows (no pandas)\n    \"\"\"\n    print(f\"üìÅ Checking CSV structure: {csv_path}\")\n    \n    # Check if file exists\n    if not os.path.exists(csv_path):\n        print(f\"‚ùå File not found: {csv_path}\")\n        return None, None\n    \n    # Read first few lines to understand structure\n    with open(csv_path, 'r', newline='', encoding='utf-8') as file:\n        reader = csv.reader(file)\n        rows = []\n        for i, row in enumerate(reader):\n            rows.append(row)\n            if i >= 4:  # Read first 5 rows\n                break\n    \n    if not rows:\n        print(f\"‚ùå Empty CSV file\")\n        return None, None\n    \n    first_row = rows[0]\n    second_row = rows[1] if len(rows) > 1 else None\n    \n    print(f\"üîç Raw file inspection:\")\n    print(f\"  First row:  {','.join(first_row)}\")\n    if second_row:\n        print(f\"  Second row: {','.join(second_row)}\")\n    \n    # Check if first row looks like data (contains .png) rather than headers\n    has_proper_headers = not any('.png' in cell for cell in first_row)\n    \n    # Count total rows\n    with open(csv_path, 'r', newline='', encoding='utf-8') as file:\n        total_rows = sum(1 for _ in csv.reader(file))\n    \n    if has_proper_headers:\n        print(f\"‚úÖ File has proper headers\")\n        data_rows = rows[1:]  # Skip header\n        headers = first_row\n        actual_data_count = total_rows - 1\n    else:\n        print(f\"‚ö†Ô∏è  File has NO proper headers - first row is data!\")\n        print(f\"üìù Using custom column names...\")\n        data_rows = rows  # All rows are data\n        headers = ['satellite_path', 'ground_path', 'duplicate_ground_path']\n        actual_data_count = total_rows\n    \n    print(f\"\\nüìä CSV Info:\")\n    print(f\"  - Total rows: {total_rows}\")\n    print(f\"  - Data rows: {actual_data_count}\")\n    print(f\"  - Columns: {len(headers)} -> {headers}\")\n    \n    print(f\"\\nüìã First 5 data rows:\")\n    for i, row in enumerate(data_rows[:5]):\n        print(f\"  Row {i+1}: {row}\")\n    \n    print(f\"\\nüîç Sample paths analysis:\")\n    if data_rows:\n        sample_row = data_rows[0]\n        for i, (header, value) in enumerate(zip(headers, sample_row)):\n            print(f\"  Column {i+1} ({header}): {value}\")\n    \n    return rows, has_proper_headers\n\ndef fix_csv_paths(input_csv_path, output_csv_path=None):\n    \"\"\"\n    Fix the CSV file paths according to the requirements (no pandas):\n    - Column 2: Remove 'input' from filename (streetview/input0026840.png ‚Üí streetview/0026840.png)\n    - Column 3: Change to polarmap/normal/input{ID}.png\n    \"\"\"\n    \n    # Check structure first (this only reads first 5 rows for inspection)\n    result = check_csv_structure(input_csv_path)\n    if result[0] is None:\n        return None\n    \n    sample_rows, has_proper_headers = result\n    \n    # Now read ALL rows from the file\n    print(f\"üîÑ Reading complete file: {input_csv_path}\")\n    with open(input_csv_path, 'r', newline='', encoding='utf-8') as file:\n        reader = csv.reader(file)\n        all_rows = list(reader)\n    \n    print(f\"üìä Complete file loaded: {len(all_rows)} total rows\")\n    \n    print(f\"\\nüîß Applying transformations...\")\n    \n    # Determine data start and headers using the complete dataset\n    if has_proper_headers:\n        headers = all_rows[0]\n        data_rows = all_rows[1:]\n        print(f\"  - Using existing headers: {headers}\")\n    else:\n        headers = ['satellite_path', 'ground_path', 'polarmap_path']\n        data_rows = all_rows  # All rows are data\n        print(f\"  - Using custom headers: {headers}\")\n    \n    # Transform the data\n    fixed_rows = []\n    \n    print(f\"  - Processing {len(data_rows)} data rows...\")\n    print(f\"  - Fixing column 2: removing 'input' from filenames AND changing .png to .jpg\")\n    print(f\"  - Fixing column 3: changing to polarmap/normal/input{{ID}}.png\")\n    \n    for row_idx, row in enumerate(data_rows):\n        if len(row) < 3:\n            print(f\"‚ö†Ô∏è  Row {row_idx + 1} has fewer than 3 columns: {row}\")\n            fixed_rows.append(row)  # Keep as-is\n            continue\n        \n        # Extract original values\n        satellite_path = row[0]  # Keep as-is\n        ground_path = row[1]     # Remove 'input'\n        third_path = row[2]      # Convert to polarmap\n        \n        # Transform column 2: Remove 'input' from filename AND change .png to .jpg\n        fixed_ground_path = ground_path.replace('input', '').replace('.png', '.jpg')\n        \n        # Transform column 3: Change to polarmap/normal/input{ID}.png\n        try:\n            # Extract filename from original path\n            original_filename = Path(third_path).name\n            fixed_third_path = f\"polarmap/normal/{original_filename}\"\n        except:\n            fixed_third_path = third_path  # Keep original if parsing fails\n        \n        # Create fixed row\n        fixed_row = [satellite_path, fixed_ground_path, fixed_third_path]\n        \n        # Add any additional columns if they exist\n        if len(row) > 3:\n            fixed_row.extend(row[3:])\n        \n        fixed_rows.append(fixed_row)\n    \n    # Show before/after comparison\n    if data_rows:\n        print(f\"\\nüìä Transformation Results:\")\n        print(f\"Original sample row:\")\n        sample_orig = data_rows[0]\n        for i, val in enumerate(sample_orig[:3]):\n            print(f\"  Col {i+1}: {val}\")\n        \n        print(f\"\\nFixed sample row:\")\n        sample_fixed = fixed_rows[0]\n        for i, val in enumerate(sample_fixed[:3]):\n            print(f\"  Col {i+1}: {val}\")\n    \n    # Determine output path\n    if output_csv_path is None:\n        input_path = Path(input_csv_path)\n        output_csv_path = Path(\"/kaggle/working\") / f\"{input_path.stem}_fixed{input_path.suffix}\"\n    \n    # Write the fixed CSV\n    with open(output_csv_path, 'w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        \n        # Write headers\n        writer.writerow(headers)\n        \n        # Write data\n        writer.writerows(fixed_rows)\n    \n    print(f\"\\n‚úÖ Fixed CSV saved to: {output_csv_path}\")\n    print(f\"üìÅ Location: Kaggle working directory (writable)\")\n    print(f\"üìä Output summary: {len(headers)} columns, {len(fixed_rows)} data rows\")\n    \n    return fixed_rows, headers\n\ndef process_train_test_csvs(train_csv_path, test_csv_path):\n    \"\"\"\n    Process both training and test CSV files (no pandas)\n    \"\"\"\n    print(\"=\"*60)\n    print(\"üöÄ Processing Training and Test CSV Files\")\n    print(\"=\"*60)\n    \n    # Process training CSV\n    print(\"\\nüìö TRAINING CSV:\")\n    train_result = fix_csv_paths(train_csv_path)\n    \n    print(\"\\n\" + \"=\"*60)\n    \n    # Process test CSV\n    print(\"\\nüß™ TEST CSV:\")\n    test_result = fix_csv_paths(test_csv_path)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"‚úÖ Both CSV files processed successfully!\")\n    \n    return train_result, test_result\n\ndef load_csv_data(csv_path):\n    \"\"\"\n    Simple utility to load CSV data back into memory (no pandas)\n    Returns: (data_rows, headers)\n    \"\"\"\n    with open(csv_path, 'r', newline='', encoding='utf-8') as file:\n        reader = csv.reader(file)\n        headers = next(reader)  # First row is headers\n        data_rows = list(reader)  # Rest are data\n    \n    return data_rows, headers\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Your actual file paths\n    train_csv = \"/kaggle/input/cvusa-subset/train-19zl.csv\"\n    test_csv = \"/kaggle/input/cvusa-subset/val-19zl.csv\"  # Note: this is val, not test\n    \n    # Check structure of both files\n    print(\"Checking CSV structures...\")\n    check_csv_structure(train_csv)\n    print(\"\\n\" + \"=\"*40)\n    check_csv_structure(test_csv)\n    \n    print(\"\\n\" + \"=\"*60)\n    \n    # Process both files - saves to /kaggle/working/\n    train_result, test_result = process_train_test_csvs(train_csv, test_csv)\n    \n    # Print the final file locations\n    print(f\"\\nüìÅ Fixed files are now available at:\")\n    print(f\"   Training: /kaggle/working/train-19zl_fixed.csv\")\n    print(f\"   Test:     /kaggle/working/val-19zl_fixed.csv\")\n    \n    # Example of loading the data back\n    print(f\"\\nüîÑ Example: Loading fixed training data...\")\n    train_data, train_headers = load_csv_data(\"/kaggle/working/train-19zl_fixed.csv\")\n    print(f\"   Headers: {train_headers}\")\n    print(f\"   Data rows: {len(train_data)}\")\n    print(f\"   Sample: {train_data[0] if train_data else 'No data'}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:39:22.887519Z","iopub.execute_input":"2025-07-20T07:39:22.888050Z","iopub.status.idle":"2025-07-20T07:39:23.166962Z","shell.execute_reply.started":"2025-07-20T07:39:22.888032Z","shell.execute_reply":"2025-07-20T07:39:23.166398Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATASET AND DATALOADER","metadata":{}},{"cell_type":"code","source":"import os\nimport csv\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport cv2\nimport numpy as np\n\nclass CVUSADataset(Dataset):\n    def __init__(self, csv_path, data_root, size=256, polar=True, is_train=True):\n        self.data_root = data_root\n        self.size = size\n        self.polar = polar\n        \n        # Load file pairs from CSV\n        self.file_pairs = []\n        \n        print(f\"üìÇ Loading dataset from: {csv_path}\")\n        \n        with open(csv_path, 'r', newline='', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            headers = next(reader)\n            for row in reader:\n                if len(row) < 3: continue\n                \n                bingmap_path, ground_path, polarmap_path = row[0], row[1], row[2]\n                satellite_relative_path = polarmap_path if self.polar else bingmap_path\n                \n                satellite_full_path = os.path.join(data_root, satellite_relative_path)\n                ground_full_path = os.path.join(data_root, ground_path)\n                \n                if os.path.exists(satellite_full_path) and os.path.exists(ground_full_path):\n                    self.file_pairs.append((satellite_full_path, ground_full_path))\n\n        print(f\"‚úÖ Found {len(self.file_pairs)} valid image pairs.\")\n        \n        # --- MODIFIED: Create separate transform pipelines ---\n        \n        # Pipeline for TARGET images (Satellite). ALWAYS without augmentation.\n        self.satellite_transform = transforms.Compose([\n            transforms.Resize((size, size)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n        ])\n\n        # Pipeline for INPUT images (Ground). Augmentation is applied ONLY if is_train=True.\n        if is_train:\n            self.ground_transform = transforms.Compose([\n                transforms.RandomApply([\n                    transforms.RandomAffine(degrees=5, translate=(0.05, 0.05)),\n                    transforms.RandomPerspective(distortion_scale=0.3, p=0.5)\n                ], p=0.5),\n                transforms.RandomApply([\n                    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n                    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n                ], p=0.5),\n                # --- End of Augmentations ---\n                transforms.ToTensor(),\n                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n            ])\n            print(f\"   -> Mode: TRAINING (applying augmentations to ground images)\")\n        else:\n            # For the validation set, the ground transform is the same as the satellite one (no augmentations).\n            self.ground_transform = self.satellite_transform\n            print(f\"   -> Mode: VALIDATION (no augmentations)\")\n\n\n    def __len__(self):\n        return len(self.file_pairs)\n\n    def __getitem__(self, idx):\n        satellite_path, ground_path = self.file_pairs[idx]\n        \n        try:\n            # Load images\n            satellite_img = Image.open(satellite_path).convert('RGB')\n            ground_img = Image.open(ground_path).convert('RGB')\n\n            sat_array = cv2.resize(np.array(satellite_img), (256, 256), interpolation=cv2.INTER_AREA)\n            ground_array = cv2.resize(np.array(ground_img), (256, 256), interpolation=cv2.INTER_AREA)\n            satellite_img = Image.fromarray(sat_array)\n            ground_img = Image.fromarray(ground_array)\n            \n            # --- MODIFIED: Apply the correct transform to each image ---\n            satellite_tensor = self.satellite_transform(satellite_img)\n            ground_tensor = self.ground_transform(ground_img)\n            \n            return {\n                \"satellite\": satellite_tensor,  # The clean, non-augmented target\n                \"ground\": ground_tensor         # The potentially augmented input\n            }\n        \n        except Exception as e:\n            print(f\"‚ùå Error loading images at index {idx}: {e}\")\n            dummy_tensor = torch.zeros(3, self.size, self.size)\n            return {\"satellite\": dummy_tensor, \"ground\": dummy_tensor}\n\n\ndef create_dataloaders(data_root=\"/kaggle/input/cvusa-subset\", batch_size=8, polar=True):\n    train_csv = \"/kaggle/working/train-19zl_fixed.csv\"\n    test_csv = \"/kaggle/working/val-19zl_fixed.csv\"\n    \n    print(\"\\nüöÄ Creating train and test datasets...\")\n    \n    # --- MODIFIED: Pass the is_train flag to the constructor ---\n    train_dataset = CVUSADataset(\n        csv_path=train_csv,\n        data_root=data_root,\n        size=256,\n        polar=polar,\n        is_train=True  # Tell the dataset to use the training pipeline (with augmentations)\n    )\n    \n    print(\"\\n\" + \"=\"*50)\n    \n    test_dataset = CVUSADataset(\n        csv_path=test_csv,\n        data_root=data_root,\n        size=256,\n        polar=polar,\n        is_train=False # Tell the dataset to use the validation pipeline (no augmentations)\n    )\n    \n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n    \n    print(f\"\\n‚úÖ Dataloaders created:\")\n    print(f\"   üìö Training: {len(train_dataset)} samples, {len(train_dataloader)} batches\")\n    print(f\"   üß™ Testing:  {len(test_dataset)} samples, {len(test_dataloader)} batches\")\n    \n    return train_dataloader, test_dataloader\n\n\n# Test the dataloaders\nif __name__ == \"__main__\":\n    train_loader, test_loader = create_dataloaders(\n        data_root=\"/kaggle/input/cvusa-subset\",\n        batch_size=20,\n        polar=True\n    )\n    \n    print(\"\\nüîç Testing train dataloader...\")\n    train_batch = next(iter(train_loader))\n    print(f\"Train batch shapes:\")\n    print(f\"  Ground: {train_batch['ground'].shape}\")\n    print(f\"  Satellite: {train_batch['satellite'].shape}\")\n    \n    print(\"\\nüîç Testing test dataloader...\")\n    test_batch = next(iter(test_loader))\n    print(f\"Test batch shapes:\")\n    print(f\"  Ground: {test_batch['ground'].shape}\")\n    print(f\"  Satellite: {test_batch['satellite'].shape}\")\n    \n    print(\"\\n‚úÖ Both dataloaders working correctly!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:51:43.291867Z","iopub.execute_input":"2025-07-20T07:51:43.292189Z","iopub.status.idle":"2025-07-20T07:52:30.635383Z","shell.execute_reply.started":"2025-07-20T07:51:43.292164Z","shell.execute_reply":"2025-07-20T07:52:30.634665Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SHAPE FIX","metadata":{}},{"cell_type":"code","source":"\"\"\"\n\nwith torch.no_grad():\n    print(\"Check both encodings:\")\n    \n    # Satellite encoding\n    sat_quant_z, _, sat_info = model.first_stage_model.encode(batch['satellite'])\n    sat_indices = sat_info[2]\n    print(f\"Satellite indices shape: {sat_indices.shape}\")\n    print(f\"Satellite indices dim: {sat_indices.dim()}\")\n    \n    # Ground encoding  \n    ground_quant_c, _, ground_info = model.cond_stage_model.encode(batch['ground'])\n    ground_indices = ground_info[2]\n    print(f\"Ground indices shape: {ground_indices.shape}\")\n    print(f\"Ground indices dim: {ground_indices.dim()}\")\n    \n    # Fix both to 2D\n    batch_size = batch['ground'].shape[0]\n    \n    if sat_indices.dim() == 1:\n        sat_tokens_per_image = sat_indices.shape[0] // batch_size\n        sat_indices_2d = sat_indices.view(batch_size, sat_tokens_per_image)\n        print(f\"Fixed satellite indices: {sat_indices_2d.shape}\")\n    \n    if ground_indices.dim() == 1:\n        ground_tokens_per_image = ground_indices.shape[0] // batch_size  \n        ground_indices_2d = ground_indices.view(batch_size, ground_tokens_per_image)\n        print(f\"Fixed ground indices: {ground_indices_2d.shape}\")\n        \n    # Test concatenation\n    cz_indices = torch.cat((ground_indices_2d, sat_indices_2d), dim=1)\n    print(f\"‚úÖ Concatenation works! Shape: {cz_indices.shape}\")\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:39:23.182445Z","iopub.status.idle":"2025-07-20T07:39:23.182763Z","shell.execute_reply.started":"2025-07-20T07:39:23.182594Z","shell.execute_reply":"2025-07-20T07:39:23.182605Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FORWARD PASS OVERWRITE","metadata":{}},{"cell_type":"code","source":"\n\n# IMPORTANT: Move model to GPU\nmodel = model.to(device)\nprint(\"Model moved to\",device)\n\ndef manual_forward_pass(model, satellite_imgs, ground_imgs):\n    \"\"\"Manual forward pass with tensor reshaping fix\"\"\"\n    # Get raw encodings and fix reshaping\n    _, z_indices_raw = model.encode_to_z(satellite_imgs)\n    _, c_indices_raw = model.encode_to_c(ground_imgs)\n    \n    batch_size = satellite_imgs.shape[0]\n    z_indices = z_indices_raw.view(batch_size, -1)\n    c_indices = c_indices_raw.view(batch_size, -1)\n    \n    # Manual forward pass logic (from STEP-CHECKPOINT)\n    cz_indices = torch.cat((c_indices, z_indices), dim=1)\n    logits, _ = model.transformer(cz_indices[:, :-1])\n    logits = logits[:, c_indices.shape[1]-1:]  # Cut off conditioning\n    target = z_indices\n    \n    return logits, target\n\nprint(\"Manual Pass created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:52:43.406900Z","iopub.execute_input":"2025-07-20T07:52:43.407192Z","iopub.status.idle":"2025-07-20T07:52:43.599217Z","shell.execute_reply.started":"2025-07-20T07:52:43.407167Z","shell.execute_reply":"2025-07-20T07:52:43.598542Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LOAD MODEL FUNCTIONS","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\nfrom datetime import datetime\nfrom taming.models.cond_transformer import Net2NetTransformer\nfrom omegaconf import OmegaConf\nimport glob\n\ndef load_saved_model(checkpoint_path, device='cuda', vqgan_checkpoint_path=None):\n    \"\"\"\n    Load a model saved with your save_model_with_timestamp function\n    \n    Args:\n        checkpoint_path: Path to the .pth checkpoint file\n        device: Device to load the model on ('cuda' or 'cpu')\n        vqgan_checkpoint_path: Optional path to VQ-GAN checkpoint (auto-detected if None)\n    \n    Returns:\n        model: Loaded model ready for inference or continued training\n        checkpoint_info: Dictionary with training information\n    \"\"\"\n    \n    # Check if checkpoint file exists\n    if not os.path.exists(checkpoint_path):\n        raise FileNotFoundError(f\"Checkpoint file not found: {checkpoint_path}\")\n    \n    # Auto-detect device if CUDA not available\n    if device == 'cuda' and not torch.cuda.is_available():\n        print(\"‚ö†Ô∏è  CUDA not available, falling back to CPU\")\n        device = 'cpu'\n    \n    # Load checkpoint\n    print(f\"Loading checkpoint from: {checkpoint_path}\")\n    try:\n        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load checkpoint: {e}\")\n    \n    # Validate checkpoint structure\n    required_keys = ['epoch', 'model_state_dict', 'model_config']\n    missing_keys = [key for key in required_keys if key not in checkpoint]\n    if missing_keys:\n        raise ValueError(f\"Checkpoint missing required keys: {missing_keys}\")\n    \n    # Print checkpoint info\n    print(f\"Checkpoint info:\")\n    print(f\"  - Epoch: {checkpoint['epoch']}\")\n    print(f\"  - Loss: {checkpoint.get('loss', 'N/A')}\")\n    print(f\"  - Timestamp: {checkpoint.get('timestamp', 'N/A')}\")\n    \n    # Validate model config\n    model_config = checkpoint['model_config']\n    required_config_keys = ['transformer_vocab_size', 'transformer_block_size', \n                           'transformer_n_layer', 'transformer_n_head', 'transformer_n_embd']\n    missing_config_keys = [key for key in required_config_keys if key not in model_config]\n    if missing_config_keys:\n        raise ValueError(f\"Model config missing required keys: {missing_config_keys}\")\n    \n    # Recreate the same configuration used in your notebook\n    config = OmegaConf.create({\n        \"first_stage_config\": {\n            \"target\": \"taming.models.vqgan.VQModel\",\n            \"params\": {\n                \"ckpt_path\": None,  # We'll load this separately\n                \"embed_dim\": 256,\n                \"n_embed\": 16384,\n                \"ddconfig\": {\n                    \"double_z\": False,\n                    \"z_channels\": 256,\n                    \"resolution\": 256,\n                    \"in_channels\": 3,\n                    \"out_ch\": 3,\n                    \"ch\": 128,\n                    \"ch_mult\": [1, 1, 2, 2, 4],\n                    \"num_res_blocks\": 2,\n                    \"attn_resolutions\": [16],\n                    \"dropout\": 0.0\n                },\n                \"lossconfig\": {\n                    \"target\": \"taming.modules.losses.DummyLoss\"\n                }\n            }\n        },\n        \"cond_stage_config\": \"__is_first_stage__\",\n        \"transformer_config\": {\n            \"target\": \"taming.modules.transformer.mingpt.GPT\",\n            \"params\": {\n                \"vocab_size\": model_config['transformer_vocab_size'],\n                \"block_size\": model_config['transformer_block_size'],\n                \"n_layer\": model_config['transformer_n_layer'],\n                \"n_head\": model_config['transformer_n_head'],\n                \"n_embd\": model_config['transformer_n_embd']\n            }\n        }\n    })\n    \n    # Create the model\n    try:\n        model = Net2NetTransformer(\n            transformer_config=config.transformer_config,\n            first_stage_config=config.first_stage_config,\n            cond_stage_config=config.cond_stage_config,\n            first_stage_key=\"satellite\",\n            cond_stage_key=\"ground\",\n            unconditional=False\n        )\n        print(\"‚úÖ Model architecture created successfully\")\n    except Exception as e:\n        raise RuntimeError(f\"Failed to create model architecture: {e}\")\n    \n    # Auto-detect VQ-GAN checkpoint path if not provided\n    if vqgan_checkpoint_path is None:\n        possible_paths = [\n            \"/kaggle/working/models/vqgan_imagenet_f16_16384.ckpt\",\n            \"/kaggle/input/*/vqgan_imagenet_f16_16384.ckpt\",\n            \"./models/vqgan_imagenet_f16_16384.ckpt\",\n            \"/content/models/vqgan_imagenet_f16_16384.ckpt\"  # For Colab\n        ]\n        \n        # Use glob for wildcard paths\n        for path_pattern in possible_paths:\n            if '*' in path_pattern:\n                matches = glob.glob(path_pattern)\n                if matches:\n                    vqgan_checkpoint_path = matches[0]\n                    break\n            elif os.path.exists(path_pattern):\n                vqgan_checkpoint_path = path_pattern\n                break\n    \n    # Load the VQ-GAN weights\n    if vqgan_checkpoint_path and os.path.exists(vqgan_checkpoint_path):\n        print(f\"Loading VQ-GAN weights from: {vqgan_checkpoint_path}\")\n        try:\n            vqgan_state = torch.load(vqgan_checkpoint_path, map_location=device, weights_only=False)\n            \n            # Check if the checkpoint has the expected structure\n            if \"state_dict\" not in vqgan_state:\n                raise ValueError(\"VQ-GAN checkpoint missing 'state_dict' key\")\n            \n            model.first_stage_model.load_state_dict(vqgan_state[\"state_dict\"], strict=False)\n            \n            # Freeze VQ-GAN parameters\n            for param in model.first_stage_model.parameters():\n                param.requires_grad = False\n            print(\"‚úÖ VQ-GAN weights loaded and frozen!\")\n            \n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Failed to load VQ-GAN weights: {e}\")\n            print(\"Model will work but VQ-GAN may not be properly initialized\")\n    else:\n        print(\"‚ö†Ô∏è  VQ-GAN checkpoint not found. You may need to:\")\n        print(\"   1. Download it again\")\n        print(\"   2. Provide the correct path via vqgan_checkpoint_path parameter\")\n        print(\"   3. Ensure the file exists in one of the expected locations\")\n    \n    # Load the trained transformer weights\n    print(\"Loading trained transformer weights...\")\n    try:\n        # Load with strict=False to handle potential key mismatches\n        missing_keys, unexpected_keys = model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n        \n        if missing_keys:\n            print(f\"‚ö†Ô∏è  Missing keys in checkpoint: {len(missing_keys)} keys\")\n            if len(missing_keys) <= 5:\n                print(f\"    {missing_keys}\")\n        if unexpected_keys:\n            print(f\"‚ö†Ô∏è  Unexpected keys in checkpoint: {len(unexpected_keys)} keys\")\n            if len(unexpected_keys) <= 5:\n                print(f\"    {unexpected_keys}\")\n        \n        print(\"‚úÖ Transformer weights loaded!\")\n        \n    except Exception as e:\n        raise RuntimeError(f\"Failed to load transformer weights: {e}\")\n    \n    # Move to device\n    try:\n        model = model.to(device)\n        print(f\"‚úÖ Model moved to {device}\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Failed to move model to {device}: {e}\")\n        print(\"Falling back to CPU...\")\n        device = 'cpu'\n        model = model.to(device)\n    \n    print(f\"‚úÖ Model loaded successfully on {device}\")\n    \n    # Return model and checkpoint info\n    checkpoint_info = {\n        'epoch': checkpoint['epoch'],\n        'loss': checkpoint.get('loss', None),\n        'timestamp': checkpoint.get('timestamp', None),\n        'model_config': checkpoint['model_config'],\n        'device': device\n    }\n    \n    return model, checkpoint_info\n\ndef load_with_optimizer(checkpoint_path, device='cuda', lr=5e-4, vqgan_checkpoint_path=None):\n    \"\"\"\n    Load model with optimizer state for continued training\n    \n    Args:\n        checkpoint_path: Path to the .pth checkpoint file\n        device: Device to load the model on\n        lr: Learning rate for optimizer (in case you want to change it)\n        vqgan_checkpoint_path: Optional path to VQ-GAN checkpoint\n    \n    Returns:\n        model: Loaded model\n        optimizer: Optimizer with loaded state\n        checkpoint_info: Dictionary with training information\n    \"\"\"\n    # Load the model first\n    model, checkpoint_info = load_saved_model(checkpoint_path, device, vqgan_checkpoint_path)\n    \n    # Create optimizer\n    try:\n        optimizer = torch.optim.AdamW(model.transformer.parameters(), lr=lr, betas=(0.9, 0.95))\n        print(\"‚úÖ Optimizer created\")\n    except Exception as e:\n        raise RuntimeError(f\"Failed to create optimizer: {e}\")\n    \n    # Load checkpoint again to get optimizer state\n    try:\n        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to reload checkpoint for optimizer: {e}\")\n    \n    # Load optimizer state\n    if 'optimizer_state_dict' in checkpoint:\n        try:\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            print(\"‚úÖ Optimizer state loaded!\")\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Failed to load optimizer state: {e}\")\n            print(\"Optimizer will use default initialization\")\n    else:\n        print(\"‚ö†Ô∏è  No optimizer state found in checkpoint\")\n    \n    return model, optimizer, checkpoint_info\n\ndef find_latest_checkpoint(directory=\"/kaggle/working/\", base_name=\"cvusa_ground2satellite\"):\n    \"\"\"\n    Find the latest checkpoint file in a directory\n    \n    Args:\n        directory: Directory to search in\n        base_name: Base name of the checkpoint files\n    \n    Returns:\n        latest_checkpoint_path: Path to the latest checkpoint\n    \"\"\"\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"Directory not found: {directory}\")\n    \n    try:\n        files = os.listdir(directory)\n    except PermissionError:\n        raise PermissionError(f\"Permission denied accessing directory: {directory}\")\n    \n    checkpoint_files = []\n    \n    for filename in files:\n        if filename.startswith(base_name) and filename.endswith('.pth'):\n            filepath = os.path.join(directory, filename)\n            try:\n                # Get file modification time\n                mtime = os.path.getmtime(filepath)\n                checkpoint_files.append((mtime, filepath, filename))\n            except OSError as e:\n                print(f\"‚ö†Ô∏è  Could not access file {filename}: {e}\")\n                continue\n    \n    if not checkpoint_files:\n        raise FileNotFoundError(f\"No checkpoint files found with base name '{base_name}' in {directory}\")\n    \n    # Sort by modification time (newest first)\n    checkpoint_files.sort(reverse=True)\n    latest_checkpoint = checkpoint_files[0][1]\n    \n    print(f\"Found {len(checkpoint_files)} checkpoint files:\")\n    for i, (_, _, filename) in enumerate(checkpoint_files[:3]):  # Show top 3\n        print(f\"  {i+1}. {filename}\")\n    if len(checkpoint_files) > 3:\n        print(f\"  ... and {len(checkpoint_files) - 3} more\")\n    \n    print(f\"Latest checkpoint: {os.path.basename(latest_checkpoint)}\")\n    \n    return latest_checkpoint\n\ndef setup_for_inference(model):\n    \"\"\"\n    Prepare model for inference\n    \n    Args:\n        model: Loaded model\n    \n    Returns:\n        model: Model in eval mode\n    \"\"\"\n    model.eval()\n    # Optionally compile for better performance (if PyTorch 2.0+)\n    try:\n        if hasattr(torch, 'compile'):\n            model = torch.compile(model)\n            print(\"‚úÖ Model compiled for better performance\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Model compilation failed: {e}\")\n    \n    return model\n\ndef get_model_info(model, checkpoint_info):\n    \"\"\"\n    Print detailed model information\n    \n    Args:\n        model: Loaded model\n        checkpoint_info: Checkpoint information dictionary\n    \"\"\"\n    print(\"\\n\" + \"=\"*50)\n    print(\"MODEL INFORMATION\")\n    print(\"=\"*50)\n    \n    # Checkpoint info\n    print(f\"Epoch: {checkpoint_info['epoch']}\")\n    print(f\"Loss: {checkpoint_info.get('loss', 'N/A')}\")\n    print(f\"Timestamp: {checkpoint_info.get('timestamp', 'N/A')}\")\n    print(f\"Device: {checkpoint_info.get('device', 'Unknown')}\")\n    \n    # Model config\n    config = checkpoint_info['model_config']\n    print(f\"\\nTransformer Configuration:\")\n    print(f\"  - Vocab size: {config['transformer_vocab_size']}\")\n    print(f\"  - Block size: {config['transformer_block_size']}\")\n    print(f\"  - Layers: {config['transformer_n_layer']}\")\n    print(f\"  - Heads: {config['transformer_n_head']}\")\n    print(f\"  - Embedding dim: {config['transformer_n_embd']}\")\n    \n    # Parameter counts\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    print(f\"\\nParameters:\")\n    print(f\"  - Total: {total_params:,}\")\n    print(f\"  - Trainable: {trainable_params:,}\")\n    print(f\"  - Frozen: {total_params - trainable_params:,}\")\n    \n    print(\"=\"*50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:52:49.526722Z","iopub.execute_input":"2025-07-20T07:52:49.527435Z","iopub.status.idle":"2025-07-20T07:52:49.550939Z","shell.execute_reply.started":"2025-07-20T07:52:49.527411Z","shell.execute_reply":"2025-07-20T07:52:49.550212Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# INFERENCE SET UP","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport os\n\ndef single_image_inference(model, ground_image_path, device=device, temperature=1.0, top_k=600, top_p=0.92, save_image=False, nameadd=\"\"):\n\n    \n    # Load and preprocess the ground image\n    ground_pil = Image.open(ground_image_path).convert('RGB')\n    \n    # Use same transform as your training\n    transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n    ])\n    \n    ground_tensor = transform(ground_pil).unsqueeze(0).to(device)  # Add batch dimension\n    \n    model.eval()\n    \n    with torch.no_grad():\n        # Encode ground image to conditioning tokens\n        ground_quant_c, _, ground_info = model.cond_stage_model.encode(ground_tensor)\n        ground_indices = ground_info[2]\n        \n        # Handle reshape only if needed (like your training code)\n        batch_size = ground_tensor.shape[0]\n        if ground_indices.dim() == 1:\n            ground_tokens_per_image = ground_indices.shape[0] // batch_size  \n            ground_indices = ground_indices.view(batch_size, ground_tokens_per_image)\n        \n        # Generate satellite tokens autoregressively\n        sequence = ground_indices  # Start with conditioning\n        satellite_seq_length = 256  # 16x16 tokens\n        \n        for i in range(satellite_seq_length):\n            logits, _ = model.transformer(sequence)\n            next_token_logits = logits[:, -1, :] / temperature\n            \n            # Use your top_k_top_p_filtering function\n            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n            \n            # Sample instead of argmax\n            probs = torch.softmax(filtered_logits, dim=-1)\n            next_token = torch.multinomial(probs, 1)\n            \n            sequence = torch.cat([sequence, next_token], dim=1)\n        \n        # Extract generated satellite tokens\n        generated_tokens = sequence[:, -satellite_seq_length:]\n        \n        # Decode using first_stage_model\n        h = w = 16\n        z_indices_spatial = generated_tokens.view(batch_size, h, w)\n        \n        # Get quantized features from codebook\n        quant_z = model.first_stage_model.quantize.embedding(z_indices_spatial)\n        quant_z = quant_z.permute(0, 3, 1, 2).contiguous()  # [batch, embed_dim, h, w]\n        \n        generated_satellite_tensor = model.first_stage_model.decode(quant_z)\n    \n    # Convert tensors back to PIL images for display\n    def tensor_to_displayable(tensor):\n        # Convert tensor to displayable format [0,1]\n        img = ((tensor.squeeze(0) + 1) / 2).cpu()\n        return img.permute(1, 2, 0).clamp(0, 1)\n    \n    # Convert to displayable format\n    ground_display = tensor_to_displayable(ground_tensor)\n    generated_display = tensor_to_displayable(generated_satellite_tensor)\n    \n    # Create visualization\n    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n    \n    # Show INPUT\n    axes[0].imshow(ground_display)\n    axes[0].set_title(\"INPUT\\n(Ground View)\", fontsize=16, fontweight='bold')\n    axes[0].axis('off')\n    \n    # Show OUTPUT\n    axes[1].imshow(generated_display)\n    axes[1].set_title(\"OUTPUT\\n(Generated Satellite)\", fontsize=16, fontweight='bold')\n    axes[1].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Convert back to PIL for return\n    to_pil = transforms.ToPILImage()\n    generated_satellite_pil = to_pil(generated_display.permute(2, 0, 1))\n\n    # MODIFIED: Create unique filename based on input image and parameters\n    if save_image:\n        # Extract filename from path (without extension)\n        input_filename = os.path.splitext(os.path.basename(ground_image_path))[0]\n        \n        # Create unique filename with parameters\n        unique_filename = f\"generated_{input_filename}_{nameadd}_temp{temperature}_k{top_k}_p{top_p}.png\"\n        \n        generated_satellite_pil.save(unique_filename)\n        print(f\"‚úÖ Generated image saved as '{unique_filename}'\")\n    \n    return generated_satellite_pil, ground_pil\n\n# Conservative: \n#single_image_inference(model, '/kaggle/input/ground-normal/0000008.jpg', temperature=0.0001, top_k=5000, top_p=1.0, save_image=True) #Deterministic\n#single_image_inference(model, '/kaggle/input/ground-normal/0000008.jpg', temperature=1.0, top_k=300, top_p=0.92, save_image=True)    #Conservativeish\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:52:55.375102Z","iopub.execute_input":"2025-07-20T07:52:55.375807Z","iopub.status.idle":"2025-07-20T07:52:55.387015Z","shell.execute_reply.started":"2025-07-20T07:52:55.375783Z","shell.execute_reply":"2025-07-20T07:52:55.386431Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SAVE AND TRAIN","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\nimport torch.optim as optim\nimport torch\nimport os\nfrom datetime import datetime\nimport torch.optim as optim\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\ndef save_model_with_timestamp(model, optimizer, epoch, loss, base_name=\"cvusa_ground2satellite\"):\n    \"\"\"Save model with automatic timestamp naming in Kaggle working directory\"\"\"\n    \n    # Kaggle working directory\n    save_dir = \"/kaggle/working/\"\n    \n    # Create timestamp\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    # Create filename\n    filename = f\"{base_name}_epoch{epoch}_loss{loss:.3f}_{timestamp}.pth\"\n    \n    # Full path\n    full_path = os.path.join(save_dir, filename)\n    \n    # Save model state\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss,\n        'timestamp': timestamp,\n        'model_config': {\n            'transformer_vocab_size': model.transformer.config.vocab_size,\n            'transformer_block_size': model.transformer.config.block_size,\n            'transformer_n_layer': model.transformer.config.n_layer,\n            'transformer_n_head': model.transformer.config.n_head,\n            'transformer_n_embd': model.transformer.config.n_embd,\n        }\n    }\n    \n    torch.save(checkpoint, full_path)\n    print(f\"‚úÖ Model saved as: {full_path}\")\n    print(f\"üìÅ Location: Kaggle working directory\")\n    return full_path\n\ndef evaluate_model(model, test_dataloader, device):\n    \"\"\"\n    Evaluate model on test set and return metrics\n    \"\"\"\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n    correct_predictions = 0\n    num_batches = 0\n    \n    with torch.no_grad():\n        for batch in test_dataloader:\n            # Move to device\n            ground_imgs = batch['ground'].to(device)\n            satellite_imgs = batch['satellite'].to(device)\n            \n            # Forward pass\n            logits, target = manual_forward_pass(model, satellite_imgs, ground_imgs)\n            \n            # Calculate loss\n            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), target.reshape(-1))\n            \n            # Calculate token accuracy\n            predictions = logits.argmax(dim=-1)\n            correct = (predictions == target).sum().item()\n            \n            # Accumulate metrics\n            total_loss += loss.item()\n            total_tokens += target.numel()\n            correct_predictions += correct\n            num_batches += 1\n    \n    # Calculate averages\n    avg_loss = total_loss / num_batches\n    token_accuracy = correct_predictions / total_tokens\n    \n    model.train()  # Switch back to training mode\n    \n    return {\n        'loss': avg_loss,\n        'token_accuracy': token_accuracy,\n        'total_tokens': total_tokens,\n        'total_batches': num_batches\n    }\n\ndef run_visual_inference_samples(model, device):\n    \"\"\"Run visual inference on sample images\"\"\"\n    print(\"üé® VISUAL INFERENCE SAMPLES:\")\n    print(\"Conservative (temp=0.8) then Balanced (temp=1.0)\")\n    \n    sample_images = [\n        '/kaggle/input/cvusa-subset/streetview/0001149.jpg',\n        '/kaggle/input/cvusa-subset/streetview/0003140.jpg', \n        '/kaggle/input/cvusa-subset/streetview/0003932.jpg',\n        '/kaggle/input/cvusa-subset/streetview/0007602.jpg',\n        '/kaggle/input/cvusa-subset/streetview/0008545.jpg',\n    ]\n    \n    for img_path in sample_images:\n        print(f\"\\n--- {os.path.basename(img_path)} ---\")\n        # Conservative - don't save during training to avoid clutter\n        single_image_inference(model, img_path, device=device, temperature=0.8, top_k=100, top_p=0.9, save_image=False)\n        # Balanced - don't save during training\n        single_image_inference(model, img_path, device=device, temperature=1.0, top_k=300, top_p=0.92, save_image=False)\n\ndef train_one_epoch(model, train_dataloader, optimizer, scaler, device):\n    \"\"\"Train for one epoch and return average loss\"\"\"\n    model.train()\n    epoch_loss = 0\n    num_batches = 0\n    \n    for batch_idx, batch in enumerate(train_dataloader):\n        # Move to device\n        ground_imgs = batch['ground'].to(device)\n        satellite_imgs = batch['satellite'].to(device)\n        \n        # Forward pass with mixed precision\n        with autocast():\n            logits, target = manual_forward_pass(model, satellite_imgs, ground_imgs)\n            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), target.reshape(-1),label_smoothing=0.1)\n        \n        # Backward pass with gradient scaling\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        \n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        scaler.step(optimizer)\n        scaler.update()\n        \n        # Track loss\n        epoch_loss += loss.item()\n        num_batches += 1\n        \n        # Print progress every 50 batches\n        if batch_idx % 50 == 0:\n            print(f\"  Batch {batch_idx}, Loss: {loss.item():.4f}\")\n    \n    return epoch_loss / num_batches\n\ndef train_model_with_evaluation(model, train_dataloader, test_dataloader, num_epochs=50, lr=5e-4, inference_check=False):\n    \"\"\"\n    Modified training with train/test split and overfitting detection\n    \"\"\"\n    \n    # Setup training parameters with weight decay\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model = model.to(device)\n    model = torch.compile(model)  # Can give 10-20% speedup\n    \n    # Optimizer with weight decay for regularization\n    optimizer = optim.AdamW(model.transformer.parameters(), lr=lr, betas=(0.9, 0.95), weight_decay=0.1) #weight decay 0.01 was not enough\n    scaler = GradScaler()\n    scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n    \n    print(f\"üöÄ Starting training for {num_epochs} epochs...\")\n    print(f\"üìä Training set: {len(train_dataloader)} batches\")\n    print(f\"üìä Test set: {len(test_dataloader)} batches\")\n    print(f\"‚öôÔ∏è  Learning rate: {lr}, Weight decay: 0.01\")\n    print(f\"üñ•Ô∏è  Training on device: {device}\")\n    \n    # Tracking variables\n    best_test_loss = float('inf')\n    previous_gap = 0\n    best_model_path = None  # Track the last saved best model to delete it\n    \n    for epoch in range(num_epochs):\n        print(f\"\\n{'='*60}\")\n        print(f\"üìÖ EPOCH {epoch + 1}/{num_epochs}\")\n        print(f\"{'='*60}\")\n        \n        # 1. TRAINING PHASE\n        print(\"üèãÔ∏è Training phase...\")\n        train_loss = train_one_epoch(model, train_dataloader, optimizer, scaler, device)\n        \n        # 2. EVALUATION PHASE  \n        print(\"üß™ Evaluation phase...\")\n        test_metrics = evaluate_model(model, test_dataloader, device)\n        test_loss = test_metrics['loss']\n        test_accuracy = test_metrics['token_accuracy']\n        \n        # 3. UPDATE LEARNING RATE\n        scheduler.step()\n        current_lr = optimizer.param_groups[0]['lr']\n        \n        # 4. OVERFITTING DETECTION (using absolute value)\n        current_gap = abs(test_loss - train_loss)  # FIXED: Use absolute value\n        gap_status = \"\"\n        if epoch > 0:  # Skip first epoch comparison\n            if current_gap > previous_gap:\n                gap_status = \"‚ö†Ô∏è  WARNING: Gap widening (potential overfitting!)\"\n            else:\n                gap_status = \"‚úÖ Gap stable/improving\"\n        previous_gap = current_gap\n        \n        # 5. PRINT EPOCH SUMMARY\n        print(f\"\\nüìä EPOCH {epoch + 1} SUMMARY:\")\n        print(f\"   üèãÔ∏è  Train Loss:     {train_loss:.4f}\")\n        print(f\"   üß™ Test Loss:      {test_loss:.4f}\")\n        print(f\"   üéØ Test Accuracy:  {test_accuracy:.3f} ({test_accuracy*100:.1f}%)\")\n        print(f\"   üìà Learning Rate:  {current_lr:.2e}\")\n        print(f\"   üìè Loss Gap (abs): {current_gap:.4f}\")\n        if gap_status:\n            print(f\"   {gap_status}\")\n        \n        # 6. BEST MODEL SAVING (when test loss improves)\n        if test_loss < best_test_loss:\n            # FIXED: Calculate improvement BEFORE updating best_test_loss\n            improvement_amount = best_test_loss - test_loss\n            improvement = f\"improved by {improvement_amount:.4f}\" if epoch > 0 else \"first save\"\n            \n            # Delete previous best model if it exists\n            if best_model_path and os.path.exists(best_model_path):\n                os.remove(best_model_path)\n                print(f\"   üóëÔ∏è  Deleted previous best model: {os.path.basename(best_model_path)}\")\n            \n            # Update best loss and save new best model\n            best_test_loss = test_loss\n            print(f\"   üèÜ New best test loss! Saving 'improve' model... ({improvement})\")\n            best_model_path = save_model_with_timestamp(model, optimizer, epoch+1, test_loss, \n                                    base_name=\"cvusa_ground2satellite_improve\")\n        \n        # 7. ROUTINE SAVING + DETAILED METRICS (every 5 epochs)\n        if (epoch + 1) % 5 == 0:\n            print(f\"\\nüìÖ EPOCH {epoch + 1} - DETAILED EVALUATION:\")\n            \n            # Calculate and display perplexity\n            perplexity = torch.exp(torch.tensor(test_loss))\n            print(f\"   üìà Test Perplexity: {perplexity:.2f}\")\n            print(f\"      (Model is as 'confused' as choosing randomly from ~{perplexity:.0f} options)\")\n            \n            # Routine save\n            print(\"   üíæ Routine save...\")\n            save_model_with_timestamp(model, optimizer, epoch+1, test_loss, \n                                    base_name=\"cvusa_ground2satellite_routine\")\n            \n            # Visual inference samples\n            if inference_check:\n                run_visual_inference_samples(model, device)\n            else:\n                print(\"Inference check not enabled\\n\")\n            \n            print(f\"   ‚úÖ Epoch {epoch + 1} detailed evaluation complete\")\n    \n    print(f\"\\nüéâ TRAINING COMPLETED!\")\n    print(f\"üèÜ Best test loss achieved: {best_test_loss:.4f}\")\n    print(f\"üìÅ All models saved in: /kaggle/working/\")\n    if best_model_path:\n        print(f\"ü•á Best model: {os.path.basename(best_model_path)}\")\n\n\nepochs = 75\nlearning_rate = 5e-4 # 5e-4 was maybe too much\n    \ntrain_model_with_evaluation(\n        model=model,  # Your existing model\n        train_dataloader=train_loader,  # New train dataloader\n        test_dataloader=test_loader,    # New test dataloader\n        num_epochs=epochs,\n        lr=learning_rate,\n        inference_check=False,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T07:39:23.189109Z","iopub.status.idle":"2025-07-20T07:39:23.189420Z","shell.execute_reply.started":"2025-07-20T07:39:23.189260Z","shell.execute_reply":"2025-07-20T07:39:23.189275Z"}},"outputs":[],"execution_count":null}]}