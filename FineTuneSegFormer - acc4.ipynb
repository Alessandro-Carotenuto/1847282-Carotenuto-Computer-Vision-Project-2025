{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12509854,"sourceType":"datasetVersion","datasetId":7895938}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# INSTALL","metadata":{}},{"cell_type":"code","source":"# Install the core libraries we need\n!pip install transformers datasets evaluate -q\n!pip install albumentations -q  # A popular library for data augmentation\n\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-20T21:44:48.059134Z","iopub.execute_input":"2025-07-20T21:44:48.059401Z","iopub.status.idle":"2025-07-20T21:44:57.826483Z","shell.execute_reply.started":"2025-07-20T21:44:48.059380Z","shell.execute_reply":"2025-07-20T21:44:57.825498Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CONFIGURATION","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\n\nclass TrainingConfig:\n    # --- Paths ---\n    # Assumes your labeled CVUSA dataset is added to Kaggle\n    dataset_path = \"/kaggle/input/cvusa-subset/polarmap\"\n    image_dir = os.path.join(dataset_path, \"normal\")\n    mask_dir = \"/kaggle/working/corrected_masks\"\n    output_dir = \"/kaggle/working/segformer-finetuned-cvusa/\"\n\n    # --- Model ---\n    # We start from a model pre-trained on Cityscapes\n    model_name = \"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\"\n    # YOU MUST CHANGE THIS: Set the number of classes in your dataset\n    # (e.g., building, road, tree, grass, background = 5 classes)\n    num_classes = 6 #CHANGED TO SIX\n\n    # --- Training ---\n    batch_size = 4  # Adjust based on your GPU memory (P100/V100 can handle more)\n    learning_rate = 6e-5\n    num_epochs = 25 # Train for more epochs for better results\n\n    # --- System ---\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Create the output directory if it doesn't exist\nos.makedirs(TrainingConfig.output_dir, exist_ok=True)\nprint(f\"Device: {TrainingConfig.device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T21:44:57.828257Z","iopub.execute_input":"2025-07-20T21:44:57.828484Z","iopub.status.idle":"2025-07-20T21:45:05.251606Z","shell.execute_reply.started":"2025-07-20T21:44:57.828452Z","shell.execute_reply":"2025-07-20T21:45:05.250950Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CSV NAMING FIXES","metadata":{}},{"cell_type":"code","source":"import csv\nimport os\nfrom pathlib import Path\n\ndef check_csv_structure(csv_path):\n    \"\"\"\n    Check the structure of the CSV file and print sample rows (no pandas)\n    \"\"\"\n    print(f\"📁 Checking CSV structure: {csv_path}\")\n    \n    # Check if file exists\n    if not os.path.exists(csv_path):\n        print(f\"❌ File not found: {csv_path}\")\n        return None, None\n    \n    # Read first few lines to understand structure\n    with open(csv_path, 'r', newline='', encoding='utf-8') as file:\n        reader = csv.reader(file)\n        rows = []\n        for i, row in enumerate(reader):\n            rows.append(row)\n            if i >= 4:  # Read first 5 rows\n                break\n    \n    if not rows:\n        print(f\"❌ Empty CSV file\")\n        return None, None\n    \n    first_row = rows[0]\n    second_row = rows[1] if len(rows) > 1 else None\n    \n    print(f\"🔍 Raw file inspection:\")\n    print(f\"  First row:  {','.join(first_row)}\")\n    if second_row:\n        print(f\"  Second row: {','.join(second_row)}\")\n    \n    # Check if first row looks like data (contains .png) rather than headers\n    has_proper_headers = not any('.png' in cell for cell in first_row)\n    \n    # Count total rows\n    with open(csv_path, 'r', newline='', encoding='utf-8') as file:\n        total_rows = sum(1 for _ in csv.reader(file))\n    \n    if has_proper_headers:\n        print(f\"✅ File has proper headers\")\n        data_rows = rows[1:]  # Skip header\n        headers = first_row\n        actual_data_count = total_rows - 1\n    else:\n        print(f\"⚠️  File has NO proper headers - first row is data!\")\n        print(f\"📝 Using custom column names...\")\n        data_rows = rows  # All rows are data\n        headers = ['satellite_path', 'ground_path', 'duplicate_ground_path']\n        actual_data_count = total_rows\n    \n    print(f\"\\n📊 CSV Info:\")\n    print(f\"  - Total rows: {total_rows}\")\n    print(f\"  - Data rows: {actual_data_count}\")\n    print(f\"  - Columns: {len(headers)} -> {headers}\")\n    \n    print(f\"\\n📋 First 5 data rows:\")\n    for i, row in enumerate(data_rows[:5]):\n        print(f\"  Row {i+1}: {row}\")\n    \n    print(f\"\\n🔍 Sample paths analysis:\")\n    if data_rows:\n        sample_row = data_rows[0]\n        for i, (header, value) in enumerate(zip(headers, sample_row)):\n            print(f\"  Column {i+1} ({header}): {value}\")\n    \n    return rows, has_proper_headers\n\ndef fix_csv_paths(input_csv_path, output_csv_path=None):\n    \"\"\"\n    Fix the CSV file paths according to the requirements (no pandas):\n    - Column 2: Remove 'input' from filename (streetview/input0026840.png → streetview/0026840.png)\n    - Column 3: Change to polarmap/normal/input{ID}.png\n    \"\"\"\n    \n    # Check structure first (this only reads first 5 rows for inspection)\n    result = check_csv_structure(input_csv_path)\n    if result[0] is None:\n        return None\n    \n    sample_rows, has_proper_headers = result\n    \n    # Now read ALL rows from the file\n    print(f\"🔄 Reading complete file: {input_csv_path}\")\n    with open(input_csv_path, 'r', newline='', encoding='utf-8') as file:\n        reader = csv.reader(file)\n        all_rows = list(reader)\n    \n    print(f\"📊 Complete file loaded: {len(all_rows)} total rows\")\n    \n    print(f\"\\n🔧 Applying transformations...\")\n    \n    # Determine data start and headers using the complete dataset\n    if has_proper_headers:\n        headers = all_rows[0]\n        data_rows = all_rows[1:]\n        print(f\"  - Using existing headers: {headers}\")\n    else:\n        headers = ['satellite_path', 'ground_path', 'polarmap_path']\n        data_rows = all_rows  # All rows are data\n        print(f\"  - Using custom headers: {headers}\")\n    \n    # Transform the data\n    fixed_rows = []\n    \n    print(f\"  - Processing {len(data_rows)} data rows...\")\n    print(f\"  - Fixing column 2: removing 'input' from filenames AND changing .png to .jpg\")\n    print(f\"  - Fixing column 3: changing to polarmap/normal/input{{ID}}.png\")\n    \n    for row_idx, row in enumerate(data_rows):\n        if len(row) < 3:\n            print(f\"⚠️  Row {row_idx + 1} has fewer than 3 columns: {row}\")\n            fixed_rows.append(row)  # Keep as-is\n            continue\n        \n        # Extract original values\n        satellite_path = row[0]  # Keep as-is\n        ground_path = row[1]     # Remove 'input'\n        third_path = row[2]      # Convert to polarmap\n        \n        # Transform column 2: Remove 'input' from filename AND change .png to .jpg\n        fixed_ground_path = ground_path.replace('input', '').replace('.png', '.jpg')\n        \n        # Transform column 3: Change to polarmap/normal/input{ID}.png\n        try:\n            # Extract filename from original path\n            original_filename = Path(third_path).name\n            fixed_third_path = f\"polarmap/normal/{original_filename}\"\n        except:\n            fixed_third_path = third_path  # Keep original if parsing fails\n        \n        # Create fixed row\n        fixed_row = [satellite_path, fixed_ground_path, fixed_third_path]\n        \n        # Add any additional columns if they exist\n        if len(row) > 3:\n            fixed_row.extend(row[3:])\n        \n        fixed_rows.append(fixed_row)\n    \n    # Show before/after comparison\n    if data_rows:\n        print(f\"\\n📊 Transformation Results:\")\n        print(f\"Original sample row:\")\n        sample_orig = data_rows[0]\n        for i, val in enumerate(sample_orig[:3]):\n            print(f\"  Col {i+1}: {val}\")\n        \n        print(f\"\\nFixed sample row:\")\n        sample_fixed = fixed_rows[0]\n        for i, val in enumerate(sample_fixed[:3]):\n            print(f\"  Col {i+1}: {val}\")\n    \n    # Determine output path\n    if output_csv_path is None:\n        input_path = Path(input_csv_path)\n        output_csv_path = Path(\"/kaggle/working\") / f\"{input_path.stem}_fixed{input_path.suffix}\"\n    \n    # Write the fixed CSV\n    with open(output_csv_path, 'w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        \n        # Write headers\n        writer.writerow(headers)\n        \n        # Write data\n        writer.writerows(fixed_rows)\n    \n    print(f\"\\n✅ Fixed CSV saved to: {output_csv_path}\")\n    print(f\"📁 Location: Kaggle working directory (writable)\")\n    print(f\"📊 Output summary: {len(headers)} columns, {len(fixed_rows)} data rows\")\n    \n    return fixed_rows, headers\n\ndef process_train_test_csvs(train_csv_path, test_csv_path):\n    \"\"\"\n    Process both training and test CSV files (no pandas)\n    \"\"\"\n    print(\"=\"*60)\n    print(\"🚀 Processing Training and Test CSV Files\")\n    print(\"=\"*60)\n    \n    # Process training CSV\n    print(\"\\n📚 TRAINING CSV:\")\n    train_result = fix_csv_paths(train_csv_path)\n    \n    print(\"\\n\" + \"=\"*60)\n    \n    # Process test CSV\n    print(\"\\n🧪 TEST CSV:\")\n    test_result = fix_csv_paths(test_csv_path)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"✅ Both CSV files processed successfully!\")\n    \n    return train_result, test_result\n\ndef load_csv_data(csv_path):\n    \"\"\"\n    Simple utility to load CSV data back into memory (no pandas)\n    Returns: (data_rows, headers)\n    \"\"\"\n    with open(csv_path, 'r', newline='', encoding='utf-8') as file:\n        reader = csv.reader(file)\n        headers = next(reader)  # First row is headers\n        data_rows = list(reader)  # Rest are data\n    \n    return data_rows, headers\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Your actual file paths\n    train_csv = \"/kaggle/input/cvusa-subset/train-19zl.csv\"\n    test_csv = \"/kaggle/input/cvusa-subset/val-19zl.csv\"  # Note: this is val, not test\n    \n    # Check structure of both files\n    print(\"Checking CSV structures...\")\n    check_csv_structure(train_csv)\n    print(\"\\n\" + \"=\"*40)\n    check_csv_structure(test_csv)\n    \n    print(\"\\n\" + \"=\"*60)\n    \n    # Process both files - saves to /kaggle/working/\n    train_result, test_result = process_train_test_csvs(train_csv, test_csv)\n    \n    # Print the final file locations\n    print(f\"\\n📁 Fixed files are now available at:\")\n    print(f\"   Training: /kaggle/working/train-19zl_fixed.csv\")\n    print(f\"   Test:     /kaggle/working/val-19zl_fixed.csv\")\n    \n    # Example of loading the data back\n    print(f\"\\n🔄 Example: Loading fixed training data...\")\n    train_data, train_headers = load_csv_data(\"/kaggle/working/train-19zl_fixed.csv\")\n    print(f\"   Headers: {train_headers}\")\n    print(f\"   Data rows: {len(train_data)}\")\n    print(f\"   Sample: {train_data[0] if train_data else 'No data'}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T21:45:05.252444Z","iopub.execute_input":"2025-07-20T21:45:05.252804Z","iopub.status.idle":"2025-07-20T21:45:05.414594Z","shell.execute_reply.started":"2025-07-20T21:45:05.252785Z","shell.execute_reply":"2025-07-20T21:45:05.413879Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CONVERSION","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\n\n# --- 1. FINALIZED CONFIGURATION ---\n\n# The source directory for your original RGB masks\nSOURCE_MASK_DIR = \"/kaggle/input/cvusa-subset/polarmap/segmap/\"\n# The target directory where the corrected, single-channel masks will be saved\nTARGET_MASK_DIR = \"/kaggle/working/corrected_masks/\"\n\n# This is your corrected and finalized color map.\n# The script will use these values to perform the conversion.\nFINAL_COLOR_MAP = {\n    # Class_ID : (R, G, B)\n    0: (255, 0, 0),        # Red\n    1: (0, 0, 255),        # Dark Blue\n    2: (255, 255, 255),    # White\n    3: (0, 255, 255),      # Light Blue (Cyan)\n    4: (0, 255, 0),        # Green\n    5: (255, 255, 0)       # Yellow\n}\n\n# --- 2. CONVERSION LOGIC (No need to edit below) ---\n\ndef convert_rgb_to_class_id(rgb_array, color_map):\n    \"\"\"Converts an RGB numpy array to a class ID array using a color map.\"\"\"\n    primary_colors = np.array(list(color_map.values()), dtype=np.uint8)\n    class_ids = np.array(list(color_map.keys()), dtype=np.uint8)\n\n    # Calculate Euclidean distance and find the closest primary color\n    distances = np.sum((rgb_array[:, :, np.newaxis, :] - primary_colors[np.newaxis, np.newaxis, :, :])**2, axis=3)\n    closest_class_indices = np.argmin(distances, axis=2)\n    \n    return class_ids[closest_class_indices].astype(np.uint8)\n\n# --- 3. AUTOMATIC CHECK AND EXECUTION ---\n\nprint(\"🚀 Starting Mask Preparation Process...\")\nos.makedirs(TARGET_MASK_DIR, exist_ok=True)\n\nsource_files = sorted(os.listdir(SOURCE_MASK_DIR))\nneeds_conversion = False\n\n# Check if conversion is required\nif not os.listdir(TARGET_MASK_DIR) or len(os.listdir(TARGET_MASK_DIR)) != len(source_files):\n    print(\"⚠️ Target directory is empty or file count mismatches. Full conversion required.\")\n    needs_conversion = True\nelse:\n    # If files exist, verify the first one to see if the color map matches.\n    print(\"🔍 Target directory found. Verifying first mask to check for correctness...\")\n    \n    first_source_path = os.path.join(SOURCE_MASK_DIR, source_files[0])\n    first_target_path = os.path.join(TARGET_MASK_DIR, source_files[0])\n\n    # Open the source image and convert it in memory\n    source_img_rgb = np.array(Image.open(first_source_path).convert(\"RGB\"))\n    expected_class_ids = convert_rgb_to_class_id(source_img_rgb, FINAL_COLOR_MAP)\n    \n    # Open the existing target image\n    existing_class_ids = np.array(Image.open(first_target_path))\n\n    # Compare if the existing mask matches what we expect from the current color map\n    if not np.array_equal(expected_class_ids, existing_class_ids):\n        print(\"❌ Verification failed! Existing masks were made with a different color map.\")\n        print(\"Full conversion is required.\")\n        needs_conversion = True\n    else:\n        print(\"✅ Verification successful! Masks are already correct and up-to-date.\")\n\n# --- Run the full conversion only if necessary ---\nif needs_conversion:\n    print(f\"\\n🔧 Converting {len(source_files)} masks from '{SOURCE_MASK_DIR}'...\")\n    \n    for filename in tqdm(source_files, desc=\"Converting All Masks\"):\n        source_path = os.path.join(SOURCE_MASK_DIR, filename)\n        \n        # Open the source RGB mask\n        rgb_mask_image = Image.open(source_path).convert(\"RGB\")\n        rgb_array = np.array(rgb_mask_image)\n        \n        # Convert to class IDs using the finalized map\n        class_id_array = convert_rgb_to_class_id(rgb_array, FINAL_COLOR_MAP)\n        \n        # Save the new single-channel (grayscale) mask as a PNG\n        new_mask_image = Image.fromarray(class_id_array)\n        target_path = os.path.join(TARGET_MASK_DIR, filename)\n        new_mask_image.save(target_path)\n        \n    print(\"\\n✅ Conversion complete!\")\n    print(f\"All corrected masks are saved in: '{TARGET_MASK_DIR}'\")\nelse:\n    print(\"\\n👍 No action needed. Your masks are ready for training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T21:45:05.416236Z","iopub.execute_input":"2025-07-20T21:45:05.416471Z","iopub.status.idle":"2025-07-20T21:49:38.381789Z","shell.execute_reply.started":"2025-07-20T21:45:05.416455Z","shell.execute_reply":"2025-07-20T21:49:38.381045Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# THE DATASET ","metadata":{}},{"cell_type":"code","source":"# Cell 3: The Dataset Class (Corrected for Segmentation)\n\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport numpy as np\nimport os\nimport csv\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nclass SegmentationCvusaDataset(Dataset):\n    \"\"\"\n    A dataset class to load satellite images and their corresponding segmentation masks,\n    using a CSV file for the train/test split and deriving mask paths from image paths.\n    \"\"\"\n    def __init__(self, csv_path, data_root, transform=None):\n        self.data_root = data_root\n        self.transform = transform\n        self.image_mask_pairs = []\n\n        print(f\"📂 Loading dataset from: {csv_path}\")\n\n        with open(csv_path, 'r', newline='', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            next(reader)  # Skip header row\n\n            for row in reader:\n                if len(row) < 3: continue\n                \n                # --- DERIVATION LOGIC ---\n                # The CSV contains the relative path to the satellite image (e.g., 'polarmap/input0000008.png')\n                satellite_relative_path = row[2]  # Assuming polarmap path is the 3rd column\n                \n                # 1. Derive the full satellite image path\n                image_full_path = os.path.join(self.data_root, satellite_relative_path)\n                \n                # 2. Derive the corresponding mask path\n                base_filename = os.path.basename(satellite_relative_path) # -> \"input0000008.png\"\n                mask_filename = base_filename.replace('input', 'output')  # -> \"output0000008.png\"\n                mask_full_path = os.path.join('/kaggle/working/corrected_masks', mask_filename)\n\n                # 3. Add the pair to our list if both files exist\n                if os.path.exists(image_full_path) and os.path.exists(mask_full_path):\n                    self.image_mask_pairs.append((image_full_path, mask_full_path))\n\n        print(f\"✅ Found {len(self.image_mask_pairs)} valid image-mask pairs.\")\n\n    def __len__(self):\n        return len(self.image_mask_pairs)\n\n    def __getitem__(self, idx):\n        image_path, mask_path = self.image_mask_pairs[idx]\n\n        image = np.array(Image.open(image_path).convert(\"RGB\"))\n        mask = np.array(Image.open(mask_path).convert(\"L\")) # Masks are single-channel\n\n        if self.transform:\n            transformed = self.transform(image=image, mask=mask)\n            image = transformed['image']\n            mask = transformed['mask']\n            # Masks must be LongTensor for segmentation loss\n            mask = mask.long()\n\n        return image, mask\n\n# --- Define Transformations ---\n# We use Albumentations because it correctly handles transforms for both images and masks.\n# As requested, we will start with no data augmentation, just resizing and normalization.\n# You can easily add augmentations here later (e.g., A.HorizontalFlip).\ntransform = A.Compose([\n    A.Resize(height=512, width=512),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T21:49:38.382487Z","iopub.execute_input":"2025-07-20T21:49:38.382775Z","iopub.status.idle":"2025-07-20T21:49:40.763827Z","shell.execute_reply.started":"2025-07-20T21:49:38.382750Z","shell.execute_reply":"2025-07-20T21:49:40.763286Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# model import","metadata":{}},{"cell_type":"code","source":"# Cell 4: Model and Optimizer Definition\n\nfrom transformers import SegformerForSemanticSegmentation\nimport torch\n\n# --- 1. Import Segformer and Load the Pre-trained Model ---\nprint(\"🧠 Loading pre-trained SegFormer model...\")\n\n# We load the model from Hugging Face, telling it to replace the final\n# classification layer with a new one matching our number of classes.\nmodel = SegformerForSemanticSegmentation.from_pretrained(\n    TrainingConfig.model_name,  # Using your lowercase config object\n    num_labels=TrainingConfig.num_classes,\n    ignore_mismatched_sizes=True # This is key to replacing the final layer\n).to(TrainingConfig.device)\n\n# --- 2. Create the Optimizer ---\n# AdamW is a standard, robust optimizer for transformer models.\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=TrainingConfig.learning_rate\n)\n\nprint(\"✅ Model and optimizer created successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T21:49:40.764552Z","iopub.execute_input":"2025-07-20T21:49:40.764871Z","iopub.status.idle":"2025-07-20T21:50:14.152957Z","shell.execute_reply.started":"2025-07-20T21:49:40.764854Z","shell.execute_reply":"2025-07-20T21:50:14.152081Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TRAINING","metadata":{}},{"cell_type":"code","source":"# At the top of your cell\nfrom torch.cuda.amp import GradScaler, autocast\n\n# Cell 5: The Full Training and Validation \n\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# --- Define paths to your CSV files and data ---\nTRAIN_CSV_PATH = \"/kaggle/working/train-19zl_fixed.csv\"\nVALID_CSV_PATH = \"/kaggle/working/val-19zl_fixed.csv\"\nDATA_ROOT = \"/kaggle/input/cvusa-subset\"\nCORRECTED_MASK_DIR = \"/kaggle/working/corrected_masks/\" # Use this if you converted your masks\n\n# --- Create Datasets and DataLoaders ---\nprint(\"\\n🚀 Creating train and validation datasets for segmentation...\")\ntrain_dataset = SegmentationCvusaDataset(\n    csv_path=TRAIN_CSV_PATH,\n    data_root=DATA_ROOT,\n    transform=transform,\n)\n\nvalid_dataset = SegmentationCvusaDataset(\n    csv_path=VALID_CSV_PATH,\n    data_root=DATA_ROOT,\n    transform=transform,\n)\n\nif len(train_dataset) == 0 or len(valid_dataset) == 0:\n    print(\"\\n❌ ERROR: One or both datasets are empty. Cannot proceed with training.\")\nelse:\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=TrainingConfig.batch_size,\n        shuffle=True\n    )\n    valid_loader = DataLoader(\n        valid_dataset,\n        batch_size=TrainingConfig.batch_size,\n        shuffle=False\n    )\n    print(f\"\\n✅ Dataloaders created successfully.\")\n\n    scaler = GradScaler() # ADDED: Create the scaler ONCE, outside the loop\n\n    # --- Start the Training ---\n    print(\"\\n--- Starting Fine-Tuning ---\")\n    for epoch in range(TrainingConfig.num_epochs):\n        # This will now work because 'model' is defined in Cell 4\n        model.train()\n        total_loss = 0\n        \n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{TrainingConfig.num_epochs}\")\n        \n        for images, masks in progress_bar:\n            images = images.to(TrainingConfig.device)\n            masks = masks.to(TrainingConfig.device)\n            \n            # ADDED: autocast context manager\n            with autocast():\n                outputs = model(pixel_values=images, labels=masks)\n                loss = outputs.loss\n            \n            optimizer.zero_grad()\n            \n            # CHANGED: Use the scaler to scale the loss and call backward()\n            scaler.scale(loss).backward()\n            \n            # CHANGED: Use the scaler to unscale gradients and call optimizer.step()\n            scaler.step(optimizer)\n            \n            # CHANGED: Update the scaler for the next iteration\n            scaler.update()\n            \n            total_loss += loss.item()\n            progress_bar.set_postfix({'loss': loss.item()})\n\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch+1} finished. Average Training Loss: {avg_loss:.4f}\")\n\n        # --- Validation Loop ---\n        model.eval()\n        total_val_loss = 0\n        with torch.no_grad():\n            for images, masks in valid_loader:\n                images = images.to(TrainingConfig.device)\n                masks = masks.to(TrainingConfig.device)\n                outputs = model(pixel_values=images, labels=masks)\n                total_val_loss += outputs.loss.item()\n        avg_val_loss = total_val_loss / len(valid_loader)\n        print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n\n        # --- Save a Checkpoint ---\n        model.save_pretrained(os.path.join(TrainingConfig.output_dir, f\"checkpoint-epoch-{epoch+1}\"))\n\n    print(\"\\n--- Fine-Tuning Finished ---\")\n    model.save_pretrained(os.path.join(TrainingConfig.output_dir, \"final_model\"))\n    print(f\"Final model saved to {os.path.join(TrainingConfig.output_dir, 'final_model')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T21:50:14.154165Z","iopub.execute_input":"2025-07-20T21:50:14.154824Z"}},"outputs":[],"execution_count":null}]}